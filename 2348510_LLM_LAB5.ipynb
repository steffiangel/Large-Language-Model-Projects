{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9079715,
          "sourceType": "datasetVersion",
          "datasetId": 5477727
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steffiangel/Large-Language-Model-Projects/blob/main/2348510_LLM_LAB5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NAME ENTITY RECOGNITION USING FINE TUNED MODELING**"
      ],
      "metadata": {
        "id": "cWPbKPh0yNOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:22:51.635793Z",
          "iopub.execute_input": "2024-08-06T14:22:51.636162Z",
          "iopub.status.idle": "2024-08-06T14:22:52.099209Z",
          "shell.execute_reply.started": "2024-08-06T14:22:51.636133Z",
          "shell.execute_reply": "2024-08-06T14:22:52.098180Z"
        },
        "trusted": true,
        "id": "cJO_XYXOdtHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:22:52.101038Z",
          "iopub.execute_input": "2024-08-06T14:22:52.101431Z",
          "iopub.status.idle": "2024-08-06T14:23:05.410621Z",
          "shell.execute_reply.started": "2024-08-06T14:22:52.101405Z",
          "shell.execute_reply": "2024-08-06T14:23:05.409661Z"
        },
        "trusted": true,
        "id": "i7pnv8eFdtHu",
        "outputId": "e5ff36ff-fa13-4fef-c424-cf1a3f9377fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import pandas as pd"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:05.412067Z",
          "iopub.execute_input": "2024-08-06T14:23:05.412381Z",
          "iopub.status.idle": "2024-08-06T14:23:08.622177Z",
          "shell.execute_reply.started": "2024-08-06T14:23:05.412354Z",
          "shell.execute_reply": "2024-08-06T14:23:08.621415Z"
        },
        "trusted": true,
        "id": "UgQHEMjAdtHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:08.623287Z",
          "iopub.execute_input": "2024-08-06T14:23:08.623745Z",
          "iopub.status.idle": "2024-08-06T14:23:08.662468Z",
          "shell.execute_reply.started": "2024-08-06T14:23:08.623719Z",
          "shell.execute_reply": "2024-08-06T14:23:08.661442Z"
        },
        "trusted": true,
        "id": "vByuzV-OdtHz",
        "outputId": "9db95ffb-011d-4195-939d-db901f83e9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/kaggle/input/named-entity-recognitionner-dataset/ner.csv'\n",
        "ner_corpus_data = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:08.665081Z",
          "iopub.execute_input": "2024-08-06T14:23:08.665388Z",
          "iopub.status.idle": "2024-08-06T14:23:09.117580Z",
          "shell.execute_reply.started": "2024-08-06T14:23:08.665363Z",
          "shell.execute_reply": "2024-08-06T14:23:09.116808Z"
        },
        "trusted": true,
        "id": "2cdtvwaodtH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_corpus_data.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:09.118674Z",
          "iopub.execute_input": "2024-08-06T14:23:09.118968Z",
          "iopub.status.idle": "2024-08-06T14:23:09.263869Z",
          "shell.execute_reply.started": "2024-08-06T14:23:09.118943Z",
          "shell.execute_reply": "2024-08-06T14:23:09.262876Z"
        },
        "trusted": true,
        "id": "G-XABEwFdtH1",
        "outputId": "1898b850-9552-4fcc-8f38-5511de3f264b"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "    Sentence #                                           Sentence  \\\n0  Sentence: 1  Thousands of demonstrators have marched throug...   \n1  Sentence: 2  Families of soldiers killed in the conflict jo...   \n2  Sentence: 3  They marched from the Houses of Parliament to ...   \n3  Sentence: 4  Police put the number of marchers at 10,000 wh...   \n4  Sentence: 5  The protest comes on the eve of the annual con...   \n\n                                                 POS  \\\n0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n1  ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...   \n2  ['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...   \n3  ['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...   \n4  ['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...   \n\n                                                 Tag  \n0  ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...  \n1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Sentence</th>\n      <th>POS</th>\n      <th>Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>Thousands of demonstrators have marched throug...</td>\n      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sentence: 2</td>\n      <td>Families of soldiers killed in the conflict jo...</td>\n      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sentence: 3</td>\n      <td>They marched from the Houses of Parliament to ...</td>\n      <td>['PRP', 'VBD', 'IN', 'DT', 'NNS', 'IN', 'NN', ...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sentence: 4</td>\n      <td>Police put the number of marchers at 10,000 wh...</td>\n      <td>['NNS', 'VBD', 'DT', 'NN', 'IN', 'NNS', 'IN', ...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sentence: 5</td>\n      <td>The protest comes on the eve of the annual con...</td>\n      <td>['DT', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'IN', 'D...</td>\n      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizerFast\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:09.265222Z",
          "iopub.execute_input": "2024-08-06T14:23:09.265848Z",
          "iopub.status.idle": "2024-08-06T14:23:09.798353Z",
          "shell.execute_reply.started": "2024-08-06T14:23:09.265814Z",
          "shell.execute_reply": "2024-08-06T14:23:09.797589Z"
        },
        "trusted": true,
        "id": "vKQQBcZ5dtH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:09.799496Z",
          "iopub.execute_input": "2024-08-06T14:23:09.799902Z",
          "iopub.status.idle": "2024-08-06T14:23:12.529557Z",
          "shell.execute_reply.started": "2024-08-06T14:23:09.799869Z",
          "shell.execute_reply": "2024-08-06T14:23:12.528664Z"
        },
        "trusted": true,
        "id": "rUDs-EJ8dtH4",
        "outputId": "fb52b7d0-ff9a-456c-ce6b-fe4adc99bb17",
        "colab": {
          "referenced_widgets": [
            "1a5c5793f86b456384ef6604478cd686",
            "c169778235014142b95afd0f7abd4621",
            "9b759d27cef943f18f4711e6f0771d4f",
            "8c9617bc830c4f00a247fe12e6636362"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a5c5793f86b456384ef6604478cd686"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c169778235014142b95afd0f7abd4621"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b759d27cef943f18f4711e6f0771d4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c9617bc830c4f00a247fe12e6636362"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tag_to_id = {\n",
        "    'O': 0,\n",
        "    'B-geo': 1,\n",
        "    'I-geo': 2,\n",
        "    'B-gpe': 3,\n",
        "    'I-gpe': 4,\n",
        "    'B-per': 5,\n",
        "    'I-per': 6,\n",
        "    'B-org': 7,\n",
        "    'I-org': 8,\n",
        "    'B-tim': 9,\n",
        "    'I-tim': 10,\n",
        "    'B-art': 11,\n",
        "    'I-art': 12,\n",
        "    'B-eve': 13,\n",
        "    'I-eve': 14,\n",
        "    'B-nat': 15,\n",
        "    'I-nat': 16\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:12.530897Z",
          "iopub.execute_input": "2024-08-06T14:23:12.531279Z",
          "iopub.status.idle": "2024-08-06T14:23:12.537842Z",
          "shell.execute_reply.started": "2024-08-06T14:23:12.531244Z",
          "shell.execute_reply": "2024-08-06T14:23:12.536769Z"
        },
        "trusted": true,
        "id": "B53ZMpiUdtH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(sentence, labels):\n",
        "    tokenized_inputs = tokenizer(sentence.split(), is_split_into_words=True, truncation=True, padding='max_length', max_length=128)\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            if word_idx < len(labels):\n",
        "                label_ids.append(tag_to_id.get(labels[word_idx], -100))\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return tokenized_inputs, label_ids"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:12.539580Z",
          "iopub.execute_input": "2024-08-06T14:23:12.540438Z",
          "iopub.status.idle": "2024-08-06T14:23:12.551354Z",
          "shell.execute_reply.started": "2024-08-06T14:23:12.540392Z",
          "shell.execute_reply": "2024-08-06T14:23:12.550566Z"
        },
        "trusted": true,
        "id": "qXCALuSLdtH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "label_ids = []\n",
        "\n",
        "for idx, row in ner_corpus_data.iterrows():\n",
        "    sentence = row['Sentence']\n",
        "    pos_tags = eval(row['POS'])\n",
        "    ner_tags = eval(row['Tag'])\n",
        "\n",
        "    tokenized_inputs, labels = tokenize_and_align_labels(sentence, ner_tags)\n",
        "    input_ids.append(tokenized_inputs['input_ids'])\n",
        "    attention_masks.append(tokenized_inputs['attention_mask'])\n",
        "    label_ids.append(labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:12.552841Z",
          "iopub.execute_input": "2024-08-06T14:23:12.553253Z",
          "iopub.status.idle": "2024-08-06T14:23:38.096030Z",
          "shell.execute_reply.started": "2024-08-06T14:23:12.553221Z",
          "shell.execute_reply": "2024-08-06T14:23:38.095248Z"
        },
        "trusted": true,
        "id": "8L7EeXfldtIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_masks = torch.tensor(attention_masks)\n",
        "label_ids = torch.tensor(label_ids)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:38.097365Z",
          "iopub.execute_input": "2024-08-06T14:23:38.097656Z",
          "iopub.status.idle": "2024-08-06T14:23:44.087255Z",
          "shell.execute_reply.started": "2024-08-06T14:23:38.097624Z",
          "shell.execute_reply": "2024-08-06T14:23:44.086450Z"
        },
        "trusted": true,
        "id": "hVQB0sQfdtIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, label_ids)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:44.088901Z",
          "iopub.execute_input": "2024-08-06T14:23:44.089344Z",
          "iopub.status.idle": "2024-08-06T14:23:44.095149Z",
          "shell.execute_reply.started": "2024-08-06T14:23:44.089309Z",
          "shell.execute_reply": "2024-08-06T14:23:44.094282Z"
        },
        "trusted": true,
        "id": "LLkX9EEBdtIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:44.098549Z",
          "iopub.execute_input": "2024-08-06T14:23:44.098914Z",
          "iopub.status.idle": "2024-08-06T14:23:44.120919Z",
          "shell.execute_reply.started": "2024-08-06T14:23:44.098890Z",
          "shell.execute_reply": "2024-08-06T14:23:44.120030Z"
        },
        "trusted": true,
        "id": "J6dZpcTadtIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Tokenized input example:\", input_ids[0])\n",
        "print(\"Labels example:\", label_ids[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:44.122235Z",
          "iopub.execute_input": "2024-08-06T14:23:44.122511Z",
          "iopub.status.idle": "2024-08-06T14:23:44.142349Z",
          "shell.execute_reply.started": "2024-08-06T14:23:44.122488Z",
          "shell.execute_reply": "2024-08-06T14:23:44.141477Z"
        },
        "trusted": true,
        "id": "KIIQ0h4cdtIH",
        "outputId": "5291b137-2eff-456d-c55f-fbdb57fd65fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Tokenized input example: tensor([  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n         1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n         3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0])\nLabels example: tensor([-100,    0,    0,    0,    0,    0,    0,    1,    0,    0,    0,    0,\n           0,    1,    0,    0,    0,    0,    0,    3,    0,    0,    0,    0,\n           0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_labels = len(tag_to_id)\n",
        "model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:44.143410Z",
          "iopub.execute_input": "2024-08-06T14:23:44.143701Z",
          "iopub.status.idle": "2024-08-06T14:23:46.607824Z",
          "shell.execute_reply.started": "2024-08-06T14:23:44.143677Z",
          "shell.execute_reply": "2024-08-06T14:23:46.607039Z"
        },
        "trusted": true,
        "id": "6OVO3TLMdtII",
        "outputId": "cc5a50f1-3279-4d3d-fba5-d86c7855675f",
        "colab": {
          "referenced_widgets": [
            "6e37af89f6fa4e429dc967d7127e819a",
            "62c2b4ef472145b28f3294e353871714"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e37af89f6fa4e429dc967d7127e819a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62c2b4ef472145b28f3294e353871714"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "total_steps = len(train_dataloader) * 3\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:46.608984Z",
          "iopub.execute_input": "2024-08-06T14:23:46.609273Z",
          "iopub.status.idle": "2024-08-06T14:23:47.367628Z",
          "shell.execute_reply.started": "2024-08-06T14:23:46.609247Z",
          "shell.execute_reply": "2024-08-06T14:23:47.366663Z"
        },
        "trusted": true,
        "id": "T2PF0kx9dtIK",
        "outputId": "6790f7f8-e6c8-4aa4-9349-d5c3b96a4bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:47.368854Z",
          "iopub.execute_input": "2024-08-06T14:23:47.369266Z",
          "iopub.status.idle": "2024-08-06T14:23:47.373829Z",
          "shell.execute_reply.started": "2024-08-06T14:23:47.369240Z",
          "shell.execute_reply": "2024-08-06T14:23:47.372943Z"
        },
        "trusted": true,
        "id": "BsJJ033DdtIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:47.375092Z",
          "iopub.execute_input": "2024-08-06T14:23:47.376150Z",
          "iopub.status.idle": "2024-08-06T14:23:47.609412Z",
          "shell.execute_reply.started": "2024-08-06T14:23:47.376115Z",
          "shell.execute_reply": "2024-08-06T14:23:47.608405Z"
        },
        "trusted": true,
        "id": "ZyTm1I1YdtIM",
        "outputId": "553b3ce5-3b45-4443-9423-ee33a4bb1ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DistilBertForTokenClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=17, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids, attention_masks, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        input_ids, attention_masks, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            label_ids = labels.cpu().numpy()\n",
        "\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(label_ids)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    print(f\"Training loss: {avg_train_loss}\")\n",
        "    print(f\"Validation loss: {avg_val_loss}\")\n",
        "\n",
        "\n",
        "    all_preds_flat = all_preds.flatten()\n",
        "    all_labels_flat = all_labels.flatten()\n",
        "\n",
        "    # filter out -100 labels padding labels\n",
        "    mask = all_labels_flat != -100\n",
        "    all_preds_flat = all_preds_flat[mask]\n",
        "    all_labels_flat = all_labels_flat[mask]\n",
        "\n",
        "    # classification report\n",
        "    report = classification_report(all_labels_flat, all_preds_flat, target_names=list(tag_to_id.keys()))\n",
        "    print(report)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:23:47.610760Z",
          "iopub.execute_input": "2024-08-06T14:23:47.611074Z",
          "iopub.status.idle": "2024-08-06T14:36:53.976073Z",
          "shell.execute_reply.started": "2024-08-06T14:23:47.611048Z",
          "shell.execute_reply": "2024-08-06T14:36:53.975107Z"
        },
        "trusted": true,
        "id": "9ZIgYAvYdtIN",
        "outputId": "df915b6d-a1f0-4d70-c322-1ee31415e407"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1: 100%|██████████| 2398/2398 [04:04<00:00,  9.81it/s, loss=0.0927]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/3\nTraining loss: 0.14327682563809802\nValidation loss: 0.10581349946868916\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "              precision    recall  f1-score   support\n\n           O       0.99      0.99      0.99    178119\n       B-geo       0.86      0.89      0.87      7674\n       I-geo       0.88      0.64      0.74      1544\n       B-gpe       0.95      0.94      0.95      3160\n       I-gpe       0.86      0.51      0.64        37\n       B-per       0.85      0.84      0.85      3459\n       I-per       0.83      0.92      0.87      3437\n       B-org       0.77      0.68      0.72      3964\n       I-org       0.79      0.64      0.71      3276\n       B-tim       0.93      0.88      0.90      3987\n       I-tim       0.87      0.72      0.79      1285\n       B-art       0.67      0.03      0.05        73\n       I-art       0.00      0.00      0.00        53\n       B-eve       0.64      0.22      0.33        73\n       I-eve       0.42      0.08      0.14        61\n       B-nat       0.43      0.07      0.12        42\n       I-nat       0.00      0.00      0.00        13\n\n    accuracy                           0.97    210257\n   macro avg       0.69      0.53      0.57    210257\nweighted avg       0.96      0.97      0.97    210257\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2: 100%|██████████| 2398/2398 [04:03<00:00,  9.83it/s, loss=0.127]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/3\nTraining loss: 0.08224792614694937\nValidation loss: 0.09914999579700332\n              precision    recall  f1-score   support\n\n           O       0.99      0.99      0.99    178119\n       B-geo       0.88      0.88      0.88      7674\n       I-geo       0.85      0.74      0.79      1544\n       B-gpe       0.96      0.94      0.95      3160\n       I-gpe       0.90      0.51      0.66        37\n       B-per       0.85      0.86      0.85      3459\n       I-per       0.84      0.93      0.88      3437\n       B-org       0.73      0.74      0.73      3964\n       I-org       0.71      0.76      0.73      3276\n       B-tim       0.92      0.89      0.91      3987\n       I-tim       0.81      0.80      0.80      1285\n       B-art       0.48      0.14      0.21        73\n       I-art       0.56      0.19      0.28        53\n       B-eve       0.54      0.29      0.38        73\n       I-eve       0.57      0.20      0.29        61\n       B-nat       0.62      0.24      0.34        42\n       I-nat       0.33      0.08      0.12        13\n\n    accuracy                           0.97    210257\n   macro avg       0.74      0.60      0.64    210257\nweighted avg       0.97      0.97      0.97    210257\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3: 100%|██████████| 2398/2398 [04:04<00:00,  9.83it/s, loss=0.0436] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/3\nTraining loss: 0.056703043737106204\nValidation loss: 0.10346151698225488\n              precision    recall  f1-score   support\n\n           O       0.99      0.99      0.99    178119\n       B-geo       0.87      0.90      0.89      7674\n       I-geo       0.80      0.81      0.80      1544\n       B-gpe       0.96      0.94      0.95      3160\n       I-gpe       0.81      0.59      0.69        37\n       B-per       0.86      0.86      0.86      3459\n       I-per       0.86      0.91      0.88      3437\n       B-org       0.78      0.72      0.75      3964\n       I-org       0.77      0.73      0.75      3276\n       B-tim       0.92      0.90      0.91      3987\n       I-tim       0.85      0.77      0.81      1285\n       B-art       0.39      0.21      0.27        73\n       I-art       0.48      0.19      0.27        53\n       B-eve       0.52      0.38      0.44        73\n       I-eve       0.47      0.30      0.36        61\n       B-nat       0.56      0.24      0.33        42\n       I-nat       0.33      0.08      0.12        13\n\n    accuracy                           0.97    210257\n   macro avg       0.72      0.62      0.65    210257\nweighted avg       0.97      0.97      0.97    210257\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.save_pretrained('/kaggle/working/fine_tuned_distilbert_ner')\n",
        "tokenizer.save_pretrained('/kaggle/working/fine_tuned_distilbert_ner')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:39:16.580001Z",
          "iopub.execute_input": "2024-08-06T14:39:16.580366Z",
          "iopub.status.idle": "2024-08-06T14:39:17.133559Z",
          "shell.execute_reply.started": "2024-08-06T14:39:16.580336Z",
          "shell.execute_reply": "2024-08-06T14:39:17.132623Z"
        },
        "trusted": true,
        "id": "yMVRoyrDdtIP",
        "outputId": "9734efa9-4a21-4af2-d559-6728591fbaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('/kaggle/working/fine_tuned_distilbert_ner/tokenizer_config.json',\n '/kaggle/working/fine_tuned_distilbert_ner/special_tokens_map.json',\n '/kaggle/working/fine_tuned_distilbert_ner/vocab.txt',\n '/kaggle/working/fine_tuned_distilbert_ner/added_tokens.json',\n '/kaggle/working/fine_tuned_distilbert_ner/tokenizer.json')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions\n"
      ],
      "metadata": {
        "id": "rsYw0uYgdtIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForTokenClassification, BertTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = '/kaggle/working/fine_tuned_distilbert_ner'\n",
        "model = DistilBertForTokenClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "id_to_tag = {\n",
        "    0: 'O',\n",
        "    1: 'B-geo',\n",
        "    2: 'I-geo',\n",
        "    3: 'B-gpe',\n",
        "    4: 'I-gpe',\n",
        "    5: 'B-per',\n",
        "    6: 'I-per',\n",
        "    7: 'B-org',\n",
        "    8: 'I-org',\n",
        "    9: 'B-tim',\n",
        "    10: 'I-tim',\n",
        "    11: 'B-art',\n",
        "    12: 'I-art',\n",
        "    13: 'B-eve',\n",
        "    14: 'I-eve',\n",
        "    15: 'B-nat',\n",
        "    16: 'I-nat'\n",
        "}\n",
        "\n",
        "# Tokenize and predict named entities in a sentence\n",
        "def predict_named_entities(sentence):\n",
        "    inputs = tokenizer(sentence.split(), is_split_into_words=True, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
        "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=2)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    predicted_tags = [id_to_tag[pred.item()] for pred in predictions[0]]\n",
        "\n",
        "    # Extract named entities\n",
        "    named_entities = []\n",
        "    for token, tag in zip(tokens, predicted_tags):\n",
        "        if tag != 'O' and token != '[PAD]':\n",
        "            named_entities.append((token, tag))\n",
        "\n",
        "    return named_entities\n",
        "\n",
        "sentence = \"Barack Obama was born in Hawaii and became the 44th President of the United States.\"\n",
        "named_entities = predict_named_entities(sentence)\n",
        "\n",
        "print(\"Named Entities:\", named_entities)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-06T14:40:45.660989Z",
          "iopub.execute_input": "2024-08-06T14:40:45.661369Z",
          "iopub.status.idle": "2024-08-06T14:40:46.005818Z",
          "shell.execute_reply.started": "2024-08-06T14:40:45.661339Z",
          "shell.execute_reply": "2024-08-06T14:40:46.004899Z"
        },
        "trusted": true,
        "id": "POcgwAy3dtIQ",
        "outputId": "2ac38800-046a-4619-f7f0-52fc011199d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Named Entities: [('barack', 'B-per'), ('obama', 'I-per'), ('hawaii', 'B-geo'), ('44th', 'B-tim'), ('united', 'B-geo'), ('states', 'I-geo')]\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}